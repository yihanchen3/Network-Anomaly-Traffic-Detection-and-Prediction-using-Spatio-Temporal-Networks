{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.layers import Dense,Conv1D,Dropout,MaxPooling1D\n",
    "from keras.layers import LSTM,Flatten,Reshape\n",
    "from attention import Attention\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error,mean_squared_log_error\n",
    "from tensorflow import compat,config\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# define basic function\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# RMSLE loss function\n",
    "from keras import backend as K\n",
    "def root_mean_squared_logarithmic_error(y_true, y_pred):\n",
    "        first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "        second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "        return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first dataset\n",
    "dataframe = pandas.read_csv('Dataset/real_1.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "# dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))\n",
    "\n",
    "'''set predict hyperparams'''\n",
    "look_back = 20 # 决定历史时间步长\n",
    "predict_step = 1 # 表示单步预测，代码中并不会用到\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\n",
    "# example the trainX\n",
    "for i in range(5):\n",
    "    for j in range(len(trainX[i])):\n",
    "        print(trainX[i,j],end=' ')\n",
    "    print('\\n')\n",
    "print(len(trainX),len(trainY))\n",
    "\n",
    "# plot the normalized dataset\n",
    "plt.plot(dataset)\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.xlabel(\"时间戳\")\n",
    "plt.ylabel(\"归一化流量值\")\n",
    "plt.title(\" 预处理后的网络流量数据集\")\n",
    "plt.savefig('example',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all other data\n",
    "file_path = r'Dataset' #set the path accordingly\n",
    "all_files = sorted(glob.glob(file_path+\"/real_*.csv\"), key=lambda x: int(re.findall(\"[0-9]+\", x)[0]))  #按数字顺序读取csv文件\n",
    "\n",
    "'''load params'''\n",
    "plot_len = []\n",
    "len_ds = 0\n",
    "file_num = 10 #在所有csv中读取多少个csv作为数据集\n",
    "plot_num = 3  #绘制多少个结果输出图\n",
    "\n",
    "print(len(trainX), len(trainY))\n",
    "for i,filename in enumerate(all_files):\n",
    "  if i == 0:\n",
    "    plot_len.append([train_size,test_size])\n",
    "    continue\n",
    "  if i >= file_num:\n",
    "    break\n",
    "  df=pandas.read_csv(filename,usecols=[1], engine='python')\n",
    "  ds = df.values\n",
    "  ds = scaler.fit_transform(ds)\n",
    "  \n",
    "  len_ds = len_ds + len(ds)\n",
    "  print('len_ds:',len_ds,end='  ')\n",
    "\n",
    "  train_size = int(len(ds) * 0.7)\n",
    "  test_size = len(ds) - train_size\n",
    "  train, test = ds[0:train_size,:], ds[train_size:len(ds),:]  \n",
    "  # 保留第2，3，4个csv文件的位置\n",
    "  if i in range(plot_num):\n",
    "    plot_len.append([train_size,test_size])\n",
    "\n",
    "  trainXt, trainYt = create_dataset(train, look_back)\n",
    "  testXt, testYt = create_dataset(test, look_back)\n",
    "  trainXt = numpy.reshape(trainXt, (trainXt.shape[0], trainXt.shape[1], 1))\n",
    "  testXt = numpy.reshape(testXt, (testXt.shape[0], testXt.shape[1], 1))\n",
    "  \n",
    "  trainX = numpy.concatenate((trainX,trainXt))\n",
    "  trainY = numpy.concatenate((trainY,trainYt))\n",
    "  testX= numpy.concatenate((testX,testXt))\n",
    "  testY = numpy.concatenate((testY,testYt))\n",
    "  print(len(trainXt),len(trainYt),len(trainX),len(trainY))\n",
    "\n",
    "print(plot_len)\n",
    "print(look_back)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model structure\n",
    "model=Sequential()\n",
    "model.add(Conv1D(16, 5, padding='same', activation='relu',input_shape=(look_back, 1),kernel_initializer='he_normal'))\n",
    "dilation_rates = [2**i for i in range(12)]\n",
    "n_filters = 10\n",
    "kernel_size = 5\n",
    "for dilation_rate in dilation_rates:\n",
    "    model.add(Conv1D(filters = n_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding='causal',\n",
    "                dilation_rate=dilation_rate))\n",
    "model.add(Dense(n_filters,activation='relu',kernel_initializer='he_normal'))\n",
    "model.add(Reshape((1,20*n_filters)))\n",
    "model.add(TCN(128, activation='tanh',return_sequences='True'))\n",
    "model.add(Attention(units=32))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=root_mean_squared_logarithmic_error, optimizer='adam')\n",
    "model.summary()\n",
    "print(look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set train params & create data save folder\n",
    "epochs = 300\n",
    "batch_size = 128\n",
    "model_name = 'paper_bad'\n",
    "\n",
    "t_start = time.time()\n",
    "Time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(t_start)))\n",
    "sTime = time.strftime(\"%m%d//%H%M_\", time.localtime(int(t_start)))\n",
    "\n",
    "path = 'predict/' + sTime + model_name\n",
    "isExists=os.path.exists(path)\n",
    "if not isExists:\n",
    "    os.makedirs(path)\n",
    "print('folder made')\n",
    "\n",
    "# monitor:监视参数，min_delta:小于此数认为不变化，mode:loss小好，acc大好，patience:n周期无提升则退出，restore_best_weights:取最优权重\n",
    "earlyStop = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, mode='min', verbose=1, restore_best_weights = True)\n",
    "# 增加validation_data参数作为验证集，添加早停止机制，训练时打乱序列顺序\n",
    "history = model.fit(\n",
    "    trainX, trainY, \n",
    "    callbacks=[earlyStop], \n",
    "    epochs=epochs, batch_size=batch_size, \n",
    "    validation_split=0.1, \n",
    "    shuffle = True,\n",
    "    verbose=1) \n",
    "print(\"train finished\")\n",
    "\n",
    "# verbose：日志显示函数，verbose = 0 为不在标准输出流输出日志信息，verbose = 1 为输出进度条记录，verbose = 2 为每一个epoch输出一行记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model\n",
    "model_file = path + '/model.h5'\n",
    "model.save(model_file)\n",
    "\n",
    "print(history.history.keys())\n",
    "# plot train loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(path+'/loss.jpg')\n",
    "plt.show()\n",
    "\n",
    "# plot val loss\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('val_loss')\n",
    "plt.savefig(path+'/val_loss.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用之前训练好的模型进行预测\n",
    "# model_path = r'D:\\Gdesign\\demo\\predict\\0510\\2225_C+C+TCN_valtest\\model.h5'\n",
    "# model=load_model(model_path,custom_objects={'TCN':TCN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY_s = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY_s = scaler.inverse_transform([testY])\n",
    "\n",
    "plt.plot(testPredict[:,0])\n",
    "plt.plot(testY_s[0])\n",
    "# evaluate performance\n",
    "trainScore_rmse = math.sqrt(mean_squared_error(trainY_s[0], trainPredict[:,0]))\n",
    "print('Train RMSE: %.4f' % (trainScore_rmse))\n",
    "testScore_rmse = math.sqrt(mean_squared_error(testY_s[0], testPredict[:,0]))\n",
    "print('Test RMSE: %.4f' % (testScore_rmse))\n",
    "\n",
    "try:\n",
    "    testScore_rmsle = math.sqrt(mean_squared_log_error(testY_s[0], testPredict[:,0]))\n",
    "    print('RMSLE: %.4f' % (testScore_rmsle))\n",
    "except(ValueError):\n",
    "    print('RMSLE: Nan (negative prediction exists)')\n",
    "except:\n",
    "    print('RMSLE: Nan (unknown error)')\n",
    "\n",
    "testScore_r2 = r2_score(testY_s[0], testPredict[:,0])\n",
    "print('R2: %.4f' % (testScore_r2))\n",
    "testScore_MAE = mean_absolute_error(testY_s[0], testPredict[:,0])\n",
    "print('MAE: %.4f' % (testScore_MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印结果图(一个csv一张图，训练数据和测试数据绘制在一张上)\n",
    "for i,j in enumerate(plot_len):\n",
    "    if i == 0:\n",
    "        train_size = j[0]\n",
    "        test_size = j[1]\n",
    "        train_j = trainY[:train_size]\n",
    "        test_j = testY[:test_size]\n",
    "        trainPredict_j = trainPredict[:train_size,:]\n",
    "        testPredict_j = testPredict[:test_size,:]\n",
    "    elif i >0:\n",
    "        train_size_t = j[0]\n",
    "        test_size_t = j[1]\n",
    "        train_j = trainY[train_size:train_size_t+train_size]\n",
    "        test_j = testY[test_size:test_size_t+test_size]\n",
    "        trainPredict_j = trainPredict[train_size:train_size_t+train_size,:]\n",
    "        testPredict_j = testPredict[test_size:test_size_t+test_size,:]\n",
    "        \n",
    "        train_szie = train_size_t+train_size\n",
    "        test_size = test_size_t+test_size\n",
    "\n",
    "    # print(numpy.shape(train_j),numpy.shape(test_j))\n",
    "    # print(numpy.shape(trainPredict_j),numpy.shape(testPredict_j))\n",
    "    ds_j = numpy.concatenate((train_j,test_j))\n",
    "    ds_j = numpy.reshape(ds_j,(len(ds_j),1))\n",
    "    # print(numpy.shape(ds_j))\n",
    "\n",
    "    trainPredictPlot = numpy.empty((len(ds_j),1))\n",
    "    trainPredictPlot[:, :] = numpy.nan\n",
    "    trainPredictPlot[:len(trainPredict_j), :] = trainPredict_j\n",
    "    testPredictPlot = numpy.empty((len(ds_j),1))\n",
    "    testPredictPlot[:, :] = numpy.nan\n",
    "    testPredictPlot[len(trainPredict_j):len(ds_j), :] = testPredict_j\n",
    "\n",
    "    ds_js = scaler.inverse_transform(ds_j)\n",
    "\n",
    "    x = numpy.arange(0, len(ds_j), 100)\n",
    "    plt.xticks(x)\n",
    "    plt.plot(ds_js,label = \"Ground Truth\")\n",
    "    plt.plot(trainPredictPlot, label = \"Train Prediction\")\n",
    "    plt.plot(testPredictPlot, label = \"Test Prediction\")\n",
    "    plt.legend(loc=\"upper left\",fontsize=14)  \n",
    "    plt.title(\"Predict Output of real_\" + str(i+1) + \".csv\")\n",
    "    # 扩大x轴长度，否则太拥挤看不清\n",
    "    plt.gca().margins(x=0)\n",
    "    plt.gcf().canvas.draw()\n",
    "    # set size\n",
    "    maxsize = 100\n",
    "    m = 0.2\n",
    "    N =len(x)\n",
    "    s = maxsize / plt.gcf().dpi * N + 2 * m\n",
    "    margin = m / plt.gcf().get_size_inches()[0]\n",
    "    plt.gcf().subplots_adjust(left=margin, right=1. - margin)\n",
    "    plt.gcf().set_size_inches(s, plt.gcf().get_size_inches()[1])\n",
    "\n",
    "    plt.savefig(\"%s%s%d%s.jpg\"%(path, \"/predict_a\",i+1,\".jpg\"), bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印结果图(一个csv一张图，只包含测试数据)\n",
    "for i,j in enumerate(plot_len):\n",
    "    if i == 0:\n",
    "        train_size = j[0]\n",
    "        test_size = j[1]\n",
    "        train_j = trainY[:train_size]\n",
    "        test_j = testY[:test_size]\n",
    "        trainPredict_j = trainPredict[:train_size,:]\n",
    "        testPredict_j = testPredict[:test_size,:]\n",
    "    elif i >0:\n",
    "        train_size_t = j[0]\n",
    "        test_size_t = j[1]\n",
    "        train_j = trainY[train_size:train_size_t+train_size]\n",
    "        test_j = testY[test_size:test_size_t+test_size]\n",
    "        trainPredict_j = trainPredict[train_size:train_size_t+train_size,:]\n",
    "        testPredict_j = testPredict[test_size:test_size_t+test_size,:]\n",
    "        \n",
    "        train_szie = train_size_t+train_size\n",
    "        test_size = test_size_t+test_size\n",
    "\n",
    "    ds_j = numpy.concatenate((train_j,test_j))\n",
    "    ds_j = numpy.reshape(ds_j,(len(ds_j),1))\n",
    "\n",
    "    ds_js = scaler.inverse_transform(ds_j)\n",
    "    train_j_s = scaler.inverse_transform([train_j])\n",
    "    test_j_s = scaler.inverse_transform([test_j])\n",
    "\n",
    "    x = numpy.arange(0, len(ds_j), 25)\n",
    "    plt.figure(dpi=100)\n",
    "    plt.xticks(x,rotation = 50)\n",
    "    plt.plot(test_j_s[0],label = \"Ground Truth\",linestyle = (0, (1, 2)),color = 'blue',alpha=0.5)\n",
    "    plt.plot(testPredict_j, label = \"Prediction\",linestyle = (0, (1, 2)) ,color= 'red',alpha=0.7)\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Web traffic Value')\n",
    "    plt.legend(loc=\"upper right\",fontsize=8)  \n",
    "    plt.savefig(\"%s%s%d%s.jpg\"%(path, \"/predict_b\",i+1,\".jpg\"), dpi =1000,bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从数据集之外读取一个新的csv进行预测，测试泛化性\n",
    "test_file = 'real_60.csv'\n",
    "df_p = pandas.read_csv('Dataset/' + test_file, usecols=[1], engine='python')\n",
    "ds_p = scaler.fit_transform(df_p.values)\n",
    "plt.plot(ds_p)\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"NormalizedValue\")\n",
    "plt.title(\" Example plot of web traffic after preprocessing \")\n",
    "plt.show()\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "testX_p, testY_p = create_dataset(ds_p, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "testX_p = numpy.reshape(testX_p, (testX_p.shape[0], testX_p.shape[1], 1))\n",
    "\n",
    "testPredict_p = model.predict(testX_p)\n",
    "\n",
    "# invert predictions\n",
    "testPredict_p = scaler.inverse_transform(testPredict_p)\n",
    "testY_p = scaler.inverse_transform([testY_p])\n",
    "# calculate root mean squared error\n",
    "testScore_p = math.sqrt(mean_squared_error(testY_p[0], testPredict_p[:,0]))\n",
    "print('RMSE: %.4f' % (testScore_p))\n",
    "\n",
    "testPredictPlot_p = numpy.empty_like(ds_p)\n",
    "testPredictPlot_p[:, :] = numpy.nan\n",
    "testPredictPlot_p[look_back:len(ds_p)-1, :] = testPredict_p\n",
    "\n",
    "# plot baseline and predictions\n",
    "x = numpy.arange(0, len(ds_p), 50)\n",
    "y = numpy.arange(0, 13, 1)\n",
    "plt.yticks(y,fontsize=15)\n",
    "plt.xticks(x,rotation = 30,fontsize= 15)\n",
    "plt.plot(scaler.inverse_transform(ds_p),label = \"Ground Truth\") #linestyle = (0, (1, 2)),color = 'blue',alpha=0.5\n",
    "plt.plot(testPredictPlot_p, label = \"Prediction\")\n",
    "plt.xlabel(\"时间戳\",fontsize=15)\n",
    "plt.ylabel(\"网络流量值\",fontsize=15)\n",
    "plt.legend(loc=\"upper left\",fontsize=15)  \n",
    "plt.title(\"CNN+TCN+A模型预测\")\n",
    "\n",
    "plt.gca().margins(x=0)\n",
    "plt.gcf().canvas.draw()\n",
    "    \n",
    "# set size\n",
    "maxsize = 30\n",
    "m = 0.2\n",
    "N =len(x)\n",
    "s = maxsize / plt.gcf().dpi * N + 2 * m\n",
    "margin = m / plt.gcf().get_size_inches()[0]\n",
    "\n",
    "plt.gcf().subplots_adjust(left=margin, right=1. - margin)\n",
    "plt.gcf().set_size_inches(s, plt.gcf().get_size_inches()[1])\n",
    "\n",
    "plt.savefig(\"%s%s.jpg\"%(path, \"/predict_c\"), bbox_inches='tight',dpi = 1000)\n",
    "plt.show()\n",
    "\n",
    "with open(path + '/result.txt','a+') as f:\n",
    "    f.write('\\n')\n",
    "    f.write(\"%s%s\" %('test score of real_60.csv:    RMSE',testScore_p))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将结果保存到tb表\n",
    "import prettytable as pt\n",
    "from contextlib import redirect_stdout\n",
    "tb = pt.PrettyTable()\n",
    "tb.field_names = [\"Structure\", \"epochs\", \"batch_size\", \"seed\",\"look back\",\"dataset num\", \"test file\" ]\n",
    "tb.add_row([model_name,epochs,batch_size,seed,look_back,file_num,test_file])\n",
    "tb1 = pt.PrettyTable()\n",
    "tb1.field_names = [\"train rmse\", \"test rmse\",\"test rmsle\",\"test r2\",\"test mae\",\"practice mse\"]\n",
    "tb1.add_row([trainScore_rmse,testScore_rmse,testScore_rmsle,testScore_r2,testScore_MAE,testScore_p])\n",
    "\n",
    "print(tb)\n",
    "print(tb1)\n",
    "with open(path + '/result.txt','w+') as f:\n",
    "    f.write(str(tb))\n",
    "    f.write('\\n')\n",
    "    f.write(str(tb1))\n",
    "    f.write('\\n')\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "555f2be1d9c011b387e733abfa29ec64711916725d592fc2e82ce1e1651d1dce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
